from typing import Any, Optional, Literal
import json
import subprocess
import os
import shutil
import re
import concurrent.futures
import uuid
import glob

import tqdm
import argparse

def get_emoji_from_output(output):
    if (match := re.search(r"^(([üí•Ô∏è‚ùåÔ∏è‚úÖÔ∏è]+)(\([^\)]*\))?) ", output, flags=re.MULTILINE)):
        return match.group(1)
    elif "TimeoutExpired" in output:
        return "‚è∞"
    elif "failed with error (deterministic) timeout at" in output:
        return "‚ù§Ô∏è"
    elif "unknown constant" in output:
        # this commonly occurs when something generated by `@[simps]` is selected as evaluation theorem
        return "not tested"
    else:
        return "not tested"

SCRATCH_DIR = "/data/user_data/jclune/tmp-premises"

parser = argparse.ArgumentParser(description="Run tactic benchmark with premises.")
parser.add_argument("--ntp_toolkit_path", type=str, default="/home/jclune/ntp-toolkit-naive", help="Path to the ntp-toolkit repository containing a tactic_benchmark script.")
parser.add_argument("--decl_names_file", type=str, default=None, help="File containing declaration names for benchmark.")
parser.add_argument("--premises_file", type=str, default=None, help="File containing retrieved premises (default: use ground truth premises).")
parser.add_argument("--out_dir", type=str, default=None, help="Output directory for results.")
parser.add_argument("--timeout", type=int, default=300, help="Timeout for each benchmark run in seconds.")
parser.add_argument("--benchmark_type", type=str, default="simp_all_with_premises", help="Type of benchmark to run.")
parser.add_argument("--k", type=int, default=8, help="Number of top premises to use.")
parser.add_argument("--k_hammerCore", type=int, default=None, help="Number of top premises to use for hammerCore (default = k).")
parser.add_argument("--max_workers", type=int, default=8, help="Number of workers for running the benchmark.")
parser.add_argument("--rerank", action="store_true", help="Enable reranking of premises.")
parser.add_argument("--pred_simp_all_hint", action="store_true", help="Enable prediction of simp_all_hint (replacing with notInSimpAll).")
parser.add_argument("--temp_premises_dir", type=str, default=None, help="A temporary directory that simulates Examples/Mathlib/TrainingDataWithPremises, if premises_file is given")
parser.add_argument("--aesopHammerPriority", type=int, default=None, help="For the flag --aesopHammerPriority to tactic_benchmark")
parser.add_argument("--aesopPremisePriority", type=int, default=None, help="For the flag --aesopPremisePriority to tactic_benchmark")
parser.add_argument("--minictx_split", type=str, default=None, help="If specified, test on a miniCTX v2 split instead of default (Mathlib)")
parser.add_argument("--minictx_skip_setup", action="store_true", help="Skip setting up ntp-toolkit, lake build, lake exe add_premises for miniCTX")
parser.add_argument("--minictx_skip_add_premises", action="store_true", help="Skip setting lake exe add_premises for miniCTX")
parser.add_argument("--tag_suffix", type=str, default=None, help="Suffix to the output json file name")
parser.add_argument("--verbose", action="store_true", help="Print proof stats in real time")

args = parser.parse_args()

ntp_toolkit_path: str = args.ntp_toolkit_path
decl_names_for_benchmark_file: str = args.decl_names_file
premises_file: str = args.premises_file
out_dir: str = args.out_dir
timeout: int = args.timeout
k: int = args.k
k_hammerCore: Optional[int] = args.k_hammerCore
benchmark_type: str = args.benchmark_type
max_workers: int = args.max_workers
rerank: bool = args.rerank
pred_simp_all_hint: bool = args.pred_simp_all_hint
temp_premises_dir: str = args.temp_premises_dir or os.path.join(SCRATCH_DIR, f"premises-{uuid.uuid4()}")
minictx_split: Optional[str] = args.minictx_split
minictx_skip_setup: bool = args.minictx_skip_setup

os.makedirs(out_dir, exist_ok=True)
out_file = os.path.join(out_dir, benchmark_type)
if premises_file is None:
    out_file += "-gt"
elif "hammer" in benchmark_type or "premise" in benchmark_type or "selector" in benchmark_type or "duper" in benchmark_type:
    out_file += f"-k{k}"
if rerank:
    out_file += "-rr"
if pred_simp_all_hint:
    out_file += "-psah"
if args.aesopHammerPriority is not None:
    out_file += f"-hp{args.aesopHammerPriority}"
if args.aesopPremisePriority is not None:
    out_file += f"-pp{args.aesopPremisePriority}"
if k_hammerCore is not None:
    out_file += f"-khc{k_hammerCore}"
if args.tag_suffix is not None:
    out_file += f"-{args.tag_suffix}"
out_file += ".json"

with open(decl_names_for_benchmark_file) as f:
    if minictx_split is None:
        decl_names_for_benchmark = json.load(f)
    else:
        minictx_entries = [json.loads(l) for l in f]
        decl_names_for_benchmark = [{"decl_name": entry["theoremName"], "module": entry["module"]} for entry in minictx_entries]

def write_premises_dir(dir, decls, results, pred_simp_all_hint: bool):
    """Build TrainingDataWithPremises-like directory but with retrieved premises from `results`
    Returns `decls` filtered to the ones with premsies found in `results`
    """
    not_found_decl_names = []
    for decl_name in results:
        if "premises" not in results[decl_name]:
            print(f"warning: premises for {decl_name} not found")
            not_found_decl_names.append(decl_name)
    decls = [e for e in decls if e["decl_name"] not in not_found_decl_names]

    shutil.rmtree(dir, ignore_errors=True)
    os.makedirs(dir, exist_ok=True)
    for entry in decls:
        decl_name = entry["decl_name"]
        module = entry["module"]
        serialized_premises = []
        for premise, hint in zip(results[decl_name]["premises"], results[decl_name]["hints"]):
            if not pred_simp_all_hint:
                hint = "notInSimpAll"
            serialized_premises.append(f"({premise}, {hint})")
        with open(os.path.join(dir, f"{module}.jsonl"), "a") as f:
            json.dump({"declName": decl_name, "declHammerRecommendation": serialized_premises}, f)  # NOTE: upstream might change name for simp_all
            f.write("\n")

    return decls

results = {d["decl_name"]: {} for d in decl_names_for_benchmark}

if minictx_split is None:
    # Build `results` mapping declaration name to premises and hints
    if premises_file is not None:
        with open(premises_file) as f:
            premises_raw = json.load(f)
            if isinstance(premises_raw, dict):
                premises_raw = premises_raw["dot"]
            # (before nov 20) for each decl, there are multiple states corresponding to the decl (now only one)
            # we assume the first state encountered in the file is the "root" initial state
            for ps_entry in premises_raw:
                decl_name = ps_entry["decl_name"]
                if decl_name in results:
                    if "premises" not in results[decl_name]:
                        premises = ps_entry["premises"]
                        # take names of top k premises
                        rank_key = "rerank_score" if rerank else "score"
                        topk_premises = [p for p in sorted(premises, key=lambda p: p[rank_key], reverse=True)[:k]]
                        results[decl_name]["premises"] = [p["corpus_id"] for p in topk_premises]
                        results[decl_name]["hints"] = [p.get("simp_all_hint", "notInSimpAll") for p in topk_premises]
                else:
                    print(f"warning: {decl_name} not benchmarked")
    else:
        # Use ground truth premises
        for entry in decl_names_for_benchmark:
            decl_name = entry["decl_name"]
            premises = results[decl_name]["premises"] = entry["gt_premises"]
            results[decl_name]["hints"] = [entry["gt_hints"][p] for p in premises]

    decl_names_for_benchmark = write_premises_dir(temp_premises_dir, decl_names_for_benchmark, results, pred_simp_all_hint)

else:
    if not minictx_skip_setup:
        # This will set up the lakefile.lean, Examples.lean, lake build, and then fail
        subprocess.run(
            ["python", "scripts/extract_repos.py", "--cwd", ntp_toolkit_path, "--config", f"configs/config_minictx_v2_{minictx_split}.json"],
            cwd=ntp_toolkit_path,
            check=False,
        )
        subprocess.run(["lake", "build"], cwd=ntp_toolkit_path, check=True)
    if not args.minictx_skip_add_premises:
        print("Adding premises")
        import_name = {
            "carleson": "Carleson",
            "foundation": "Foundation",
        }.get(minictx_split, minictx_split)
        subprocess.run(
            ["lake", "exe", "add_premises", import_name],
            cwd=ntp_toolkit_path,
            check=True
        )

    if premises_file is None:  # use gt premises; if don't want this then set e.g. --premises_file ""
        # TODO hardcoded path
        premises_jsonl_dir = f"/home/jclune/ntp-toolkit-naive/Examples/{minictx_split.capitalize()}/TrainingDataWithPremises/"
        assert os.path.isdir(premises_jsonl_dir)
        for fname in glob.glob(os.path.join(premises_jsonl_dir, "*.jsonl")):
            with open(fname) as f:
                for line in f:
                    premises_data = json.loads(line)
                    decl_name = premises_data["declName"]
                    if decl_name in results and "premises" not in results[decl_name]:  # assume first state encountered with this decl_name is the "root state"
                        recommendation: list[str] = premises_data["declHammerRecommendation"]
                        results[decl_name]["premises"] = [r[1:r.index(',')] for r in recommendation]
                        results[decl_name]["hints"] = [r[r.index(',') + 1:-1].strip() for r in recommendation]
        # for some reason TrainingDataWithPremises misses some theorems, so we also use Premises as a backup
        premises_jsonl_dir = f"/home/jclune/ntp-toolkit-cmu-l3/Examples/{minictx_split}/Premises/"
        assert os.path.isdir(premises_jsonl_dir)
        for fname in glob.glob(os.path.join(premises_jsonl_dir, "*.jsonl")):
            with open(fname) as f:
                for line in f:
                    premises_data = json.loads(line)
                    decl_name = premises_data["name"]
                    if decl_name in results and "premises" not in results[decl_name]:
                        results[decl_name]["premises"] = [d["name"] for d in premises_data["dependents"]]
                        results[decl_name]["hints"] = ["notInSimpAll" for _ in premises_data["dependents"]]
        decl_names_for_benchmark = write_premises_dir(temp_premises_dir, decl_names_for_benchmark, results, pred_simp_all_hint)
    else:
        # retrieve in real time (e.g. hammer_nosimp)
        assert "hammerCore" not in benchmark_type


def run_benchmark(entry: dict, print_emoji: bool = False) -> dict[str, str]:
    decl_name = entry["decl_name"]
    module = entry["module"]
    result_data = {"decl_name": decl_name, "module": module}
    command: list[str] = [
        "lake", "exe", "tactic_benchmark",
        module, decl_name,
        os.path.abspath(temp_premises_dir),
        benchmark_type,
        "--k", f"{k}"
    ]
    if args.aesopHammerPriority is not None:
        command.extend(["--aesopHammerPriority", str(args.aesopHammerPriority)])
    if args.aesopPremisePriority is not None:
        command.extend(["--aesopPremisePriority", str(args.aesopPremisePriority)])
    if k_hammerCore is not None:
        command.extend(["--hammerCoreK", str(k_hammerCore)])
    if minictx_split is not None:
        command.extend(["--withImportsPath", f"Examples/{minictx_split.capitalize()}/WithImports"])  # capitalize is a leftover from older ntp-toolkit that is kept in josh's fork
    result_data["command"] = " ".join(command)
    try:
        result = subprocess.run(
            command,
            cwd=ntp_toolkit_path,
            check=False,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            timeout=timeout
        )
        result_output = "\n".join(
            line for line in result.stdout.splitlines()
            if not any(line.startswith(prefix) for prefix in ["note:", "warning:", "error:", "‚ö†", "‚úñ", "‚úî"])
        )
        result_emoji = get_emoji_from_output(result_output)
    except subprocess.TimeoutExpired as e:
        result_emoji = "‚è∞"
        result_output = str(e)

    result_data["result_emoji"] = result_emoji
    result_data["result_output"] = result_output
    # print(result_output)

    if print_emoji:
        print(result_emoji)
    return result_data


subprocess.run(
    ["lake", "build", "tactic_benchmark"],
    cwd=ntp_toolkit_path,
    check=True,
    stdout=subprocess.DEVNULL,
    stderr=subprocess.DEVNULL,
)
with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
    futures = {executor.submit(run_benchmark, entry): entry for entry in decl_names_for_benchmark}
    for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(decl_names_for_benchmark)):
        result_data = future.result()
        results[result_data["decl_name"]].update(result_data)
        if args.verbose:
            print("=" * 20)
            print(result_data)

with open(out_file, "w") as f:
    json.dump(results, f, indent=4)
print(f"Results saved to {out_file}")

# Sometimes timeout tactics leave zombie threads (TODO)
subprocess.run(
    ["killall", "tactic_benchmark"],
    check=False,
)
shutil.rmtree(temp_premises_dir, ignore_errors=True)
